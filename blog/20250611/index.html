<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <style type="text/css">
    
    body {
      font-family: "Times New Roman", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-weight: 200;
      font-size: 16px;
      margin-left: auto;
      margin-right: auto;
      width: 800px;
    }

    h1, h2{
      font-weight: 400;
      margin-top: 1.0em;
      margin-bottom: 1.0em;
    }

    h3{
      font-weight: 400;
      margin-top: 0.5em;
      margin-bottom: 0.5em;
    }

    p {
      font-size: 18px;
      font-weight: 200;
      line-height: 1.4;
    }

    code {
      font-size: 0.8rem;
      margin: 0 0.2rem;
      padding: 0.5rem 0.8rem;
      white-space: nowrap;
      background: #efefef;
      border: 1px solid #d3d3d3;
      color: #000000;
      border-radius: 3px;
    }

    pre>code {
      display: block;
      white-space: pre;
      line-height: 1.5;
      padding: 0;
      margin: 0;
    }

    pre.prettyprint>code {
      border: none;
    }

    .container {
      display: flex;
      align-items: center;
      justify-content: center
    }

    .image {
      flex-basis: 40%
    }

    .text {
      padding-left: 20px;
      padding-right: 20px;
    }

    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
      padding: 20px;
    }

    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.rounded {
      border: 0px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;

    }

    a:link,
    a:visited {
      color: #1367a7;
      text-decoration: none;
    }

    a:hover {
      color: #208799;
    }

    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }

    hr {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }

    .abstract-container {
      background-color: #f8f9fa;
      padding: 0.5em;
      border-radius: 5px;
      margin: 0.5em 0;
    }

  </style>

<link rel="icon" href="./image/0.png">
  <title>Darwin Godel Machine</title>
</head>

<body data-new-gr-c-s-check-loaded="14.1093.0" data-gr-ext-installed="">
  <center>
    <span style="font-size:36px; color:#5C5DBD; font-family: Varela Round,sans-serif; font-weight: 700; line-height: 65px;">Share the Best: I</span>
    <br>
    <span style="font-size:28px">Gaining Inspiration from Darwinian Evolution: Towards Self-Referential and Self-Improving AI System</span>
    <br>
    <br>
  </center>

  <hr>
  
  <br>

  <center> <img src="./image/1.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a> </center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <left>
      Illustration of incorporating Darwinian exploration into Gödel machine.
    </left>
  </p>

  <center> <h2> <strong> Beginning </strong> </h2> </center>
  <div class="abstract-container">
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <left>
          One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. This idea, also known as a Gödel Machine, was proposed by Jürgen Schmidhuber decades ago, which is a hypothetical self-improving AI. 
          Recently, researchers from University of British Columbia, Vector Institute, Sakana AI, and Canada CIFAR AI Chair proposed a system (Darwin Gödel Machine, DGM) that harnesses the principles of open-ended algorithms like Darwinian evolution to search for improvements that empirically improve performance.
          DGMs leverage foundation models to propose code improvements, and use recent innovations in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. 
          Experiments show that DGMs improve themselves the more compute they are provided. In line with the clear trend that AI systems that rely on learning ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems.
        </left>
      </p>
  </div>

  <br>
  <hr>

  <center> <h2> <strong> The Evolution Pipeline </strong> </h2> </center>
  <center> <img class="center" src="./image/2.png" width="800px"></p> </center>

  <p style="text-align:justify; text-justify:inter-ideograph;">
    <left>
     The Darwin Gödel Machine iteratively builds a growing archive of agents. 
     New agents are created and scored by interleaving self-modification with downstream task evaluation.
    </left>
  </p>
  <br>

  <p>This first DGM is a coding agent that has the ability to:</p>
  <ul>
  <li><strong>Read and Modify Its Own Code:</strong> It understands and can modify its own Python codebase to try to self-improve (e.g., adding a new tool, or suggesting a different workflow).</li>
  <li><strong>Evaluate if the Change Improves Performance:</strong> Proposed new versions of itself are evaluated on coding benchmarks (like SWE-bench and Polyglot).</li>
  <li><strong>Open-endedly Explore the AI Design Space:</strong> New agents are added to an ever-expanding archive of interesting agents. Harnessing the power of open-ended algorithms, future self-modifications can then branch off from any agent in this growing archive, allowing for parallel exploration of many different evolutionary paths.</li>
  </ul>

  <br>

  <center> <img class="center" src="./image/3.png" width="800px"> </center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <center> <strong style="font-weight: 900; font-size: 18px"> Data sources and task category statistics visualization of SpatialScore. </strong> </center>
  </p>

  <br>

  <center> <img class="center" src="./image/4.png" width="800px"> </center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <center> <strong style="font-weight: 900; font-size: 18px"> Data sources and task category statistics visualization of VGBench. </strong> </center>
  </p>

  <br>
  <hr>

  <center> <h2> <strong> SpatialAgent Architecture </strong> </h2> </center>
  <center> <img class="center" src="./resources/architecture.png" width="800px"> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Architecture and Workflow of SpatialAgent. </strong>
      (a) Specialized spatial understanding tools integrated in SpatialAgent;
      (b) The <i>Plan-Execute</i> paradigm for hierarchical task decomposition and stepwise execution;
      (c) The <i>ReAct</i> paradigm for iterative interaction and dynamic strategy refinement.
    </left>
  </p>

  <br>
  <hr>

  <center> <h2> <strong> Results </strong> </h2> </center>
  <h3> <center> <strong> Quantitative Results </strong> </center> </h3>
  
  <center> <img class="center" src="./resources/quantitative_results_1.png" width="800px"></p> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Quantitative Results on SpatialScore. </strong>
      Here, Count., Obj-Loc., Pos-Rel., Dist., Obj-Prop., and Cam.&IT. refer to Counting, Object Localization, 3D Positional Relation, Depth & Distance, Object Properties, and Camera & Image Transformation, respectively.
      Results with the best and second best results are <strong>bolded</strong> and <u>underlined</u>, respectively.
    </left>
  </p>

  <br>
  
  <center> <img class="center" src="./resources/quantitative_results_2.png" width="800px"> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Quantitative Results on SpatialScore-Hard. </strong>
      Our SpatialAgent demonstrates substantially greater performance improvements on this carefully curated, challenging subset, highlighting its specialized capabilities for spatial understanding tasks.
    </left>
  </p>

  <br>
  
  <center> <img class="center" src="./resources/quantitative_results_3.png" width="800px"> </center>
  <p>
    <left>
      <strong style="font-weight: 900"> Quantitative Results on VGBench. </strong>
      Here, Homo., Pose-Est., 3D-Recon., Tracking, and Obj-Pos. denote Homography Matrix, Pose Estimation, 3D Reconstruction, Point Tracking, and Object Position, respectively.
      Results with the best and second best results are are <strong>bolded</strong> and <u>underlined</u>.
    </left>
  </p>

  <br>

  <h3> <center> <strong> Qualitative Results </strong> </center> </h3>
  <p> <img class="center" src="./resources/qualitative_results.png" width="800px"> </p>
  <p>
    <left>
      <strong style="font-weight: 900"> Qualitative Results. </strong>
      We present the comprehensive reasoning process of SpatialAgent against the direct responses of other models.
      While occasional errors occur due to tool execution or interpretation mistakes, these limitations are expected to diminish as MLLMs continue to advance.
    </left>
  </p>

  <br>

  <hr>

  <center> <h2> <strong> References </strong> </h2> </center>
  <p> [1] Zhang, Jenny, et al. "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents." arXiv preprint arXiv:2505.22954 (2025).</p>
  <p> [2] https://sakana.ai/dgm/ </p>

  <br>
  <br>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>

</html>