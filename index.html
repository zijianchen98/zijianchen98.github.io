<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zijian Chen</title>

  <meta name="author" content="Zijian Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:1%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zijian Chen 「陈子健」</name>
                  </p>
                  <p>
                    I am currently a second-year PhD Student of <a href="https://multimedia.sjtu.edu.cn/">Multimedia Lab</a> at <a href="https://sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>, advised by <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=24&infoid=66&flag=66">Prof. Guangtao Zhai</a> and <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Prof. Wenjun Zhang</a>.
                    Previously, I received my M.E. degree from <a href="https://www.ecust.edu.cn/">East China University of Science and Technology</a> in June 2023, and B.S. degree in EE from <a href="https://www.wzu.edu.cn/">Wenzhou University</a> in June 2020. </p>
                  <p>
                    I'm generally interested in <strong>image/video quality assessment</strong>, <strong>large multimodal models</strong>, especially <strong>all-round evaluation</strong>, and AI4Humanity (Oracle bone character processing).
                    My ultimate goal is to build models with human-like general intelligence that can seamlessly understand, generate, and reason across multiple modalities.
                  </p>  
                  <p>
                    <font color="red"> I'm always eager to communicate and cooperate, so feel free to contact me!!! </font>
                  </p>

                  <p>
                    Email: zijian.chen@sjtu.edu.cn  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   <!-- WeChat: xxxxx_ --> 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:zijian.chen@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <!-- <a href="data/xxxxx_CV.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?hl=en&user=NSR4UkMAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/zijianchen98/">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/amorzhu-ling-feng">Zhihu</a> &nbsp/&nbsp
                    <a href="https://aiben.ch/rank">Team Web</a>
                    <!-- <a href="https://www.linkedin.com/in/xxxxx/">LinkedIn</a> -->
                  </p>
                </td>
                <td style="padding:1%;width:30%;max-width:30%">
                  <a href="images/czj.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/czj.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> [2026.01] Our PictOBI-20k paper has been accepted by <strong><a href="https://2026.ieeeicassp.org/">ICASSP 2026</a></strong>! </li>
                    <li> [2025.12] We proposed a containmation-free benchmark (<strong><a href="https://github.com/Rongdingyi/LiveProteinBench">LiveProteinBench</a></strong>) for protein biology.</li>
                    <li> [2025.12] A comprehensive survey on <strong><a href="https://www.authorea.com/users/1009838/articles/1369823-oracle-bone-inscriptions-information-processing-a-comprehensive-survey">Oracle bone inscriptions information processing</a></strong> is released! </li>
                    <li> [2025.11] Two preprints about <strong><a href="https://arxiv.org/abs/2511.10691">SQUID GAME</a></strong> and <strong><a href="https://arxiv.org/abs/2511.09139">MACEval</a></strong> are released! </li>
                    <li> [2025.09] <strong><a href="https://github.com/OBI-Future/PictOBI-20k">PictOBI-20k</a></strong>, our new exploration for MLLMs on the visual decipherment of OBI, is released! 
                    <li> [2025.07] One paper (<strong><a href="https://github.com/OBI-Future/Oracle-P15K">Oracle-P15K</a></strong>) has been accepted by <strong><a href="https://acmmm2025.org/">ACM MM 2025</a></strong> and is recommended as <strong>Oral</strong>. </li>
                    <li> [2025.05] Welcome to our <strong><a href="https://aiben.ch/rank">AIBench</a></strong> to explore the latest model rankings and uncover valuable insights!
                    <li> [2025.04] Start internship at <strong><a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a></strong> as a research intern, Shanghai (on-site). </li>
                    <li> [2025.04] I will attend <strong><a href="https://iclr.cc/">ICLR 2025</a></strong> in person. See you in Singapore! </li>
                    <li> [2025.02] One paper about (<strong><a href="https://ieeexplore.ieee.org/abstract/document/10896716">image debanding</a></strong>) has been accepted by <strong><a href="https://ieee-cas.org/publication/tcsvt">IEEE TCSVT 2025</a></strong>. </li>
                    <li> [2025.01] One paper (<strong><a href="https://github.com/zijianchen98/OBI-Bench">OBI-Bench</a></strong>) has been accepted by <strong><a href="https://iclr.cc/">ICLR 2025</a></strong>. </li>
                    <li> [2024.11] One paper (<strong><a href="https://github.com/zijianchen98/AGIN">AGIN</a></strong>), has been accepted by <strong><a href="https://ieee-cas.org/publication/tcsvt">IEEE TCSVT 2025</a></strong>. </li>
                    <li> [2024.09] One paper (<strong><a href="https://github.com/zijianchen98/GAIA">GAIA</a></strong>) has been accepted by <strong><a href="https://neurips.cc/Conferences/2024">NeurIPS 2024 D&B Track</a></strong> and selected as <strong>Spotlight</strong>. </li>
                    <li> [2024.02] One paper (<a href="https://ieeexplore.ieee.org/abstract/document/10438477">BAND-2k</a>) has been accepted by <a href="https://ieee-cas.org/publication/tcsvt">IEEE TCSVT 2024</a>. </li>
                    <li> [2024.01] One paper (<a href="https://ieeexplore.ieee.org/abstract/document/10558429">FS-BAND</a>) has been accepted by <a href="https://2024.ieee-iscas.org/">ISCAS 2024</a>. </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Preprints</heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="/images/liveproteinbench.png" alt="LiveProteinBench" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2512.22257">
                    <papertitle>LiveProteinBench: A Contamination-Free Benchmark for Assessing Models' Specialized Capabilities in Protein Science</papertitle> 
                  </a>
                  <br>
                 Dingyi Rong*, <strong>Zijian Chen*</strong>, Qi Jia*, Kaiwei Zhang, Haotian Liu, Guangtao Zhai<sup>†</sup>, Ning Liu<sup>†</sup></a>
                 <br> 
                 <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/Rongdingyi/LiveProteinBench">Repo.</a>
                  /
                  <a href="https://arxiv.org/abs/2512.22257">arxiv</a>
                  <br>
                  <p>In this paper, we introduce <strong>LiveProteinBench</strong>, a contamination-free, multimodal benchmark of 12 tasks for evaluating LLM performance on protein property and function prediction.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="/images/OBI-Survey.jpg" alt="OBI Survey" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://www.authorea.com/users/1009838/articles/1369823-oracle-bone-inscriptions-information-processing-a-comprehensive-survey">
                    <papertitle>Oracle Bone Inscriptions Information Processing: A Comprehensive Survey</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, Wenjie Hua, Jinhao Li, Yucheng Zhu, Xiaona Zhi, Zhiji Liu, Tingzhu Chen<sup>†</sup>,Wenjun Zhang, Guangtao Zhai<sup>†</sup></a>
                 <br> 
                 <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/OBI-Future/OBI-Survey">Repo.</a>
                  /
                  <a href="https://www.authorea.com/users/1009838/articles/1369823-oracle-bone-inscriptions-information-processing-a-comprehensive-survey">TechRxiv</a>
                  <br>
                  <p> We conduct a comprehensive survey into <strong>Oracle bone inscriptions information processing</strong> works over the past 20 years, reviewing more than 150 related articles.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="/images/squid_game.png" alt="Squid Game" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2511.10691">
                    <papertitle>Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, Wenjun Zhang, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                 <br> 
                 <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/zijianchen98/LLM_Squid_Game">Repo.</a>
                  /
                  <a href="https://arxiv.org/abs/2511.10691">arXiv</a>
                  <br>
                  <p> In this paper, we introduce <strong>SQUID GAME</strong>, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="/images/MACEval.png" alt="MACEval" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2511.09139">
                    <papertitle>MACEval: A Multi-Agent Continual Evaluation Network for Large Models</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, Yuze Sun, Yuan Tian, Wenjun Zhang, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                 <br> 
                 <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/zijianchen98/MACEval">Repo.</a>
                  /
                  <a href="https://arxiv.org/abs/2511.09139">arXiv</a>
                  <br>
                  <p> In this paper, we introduce <strong>MACEval</strong>, a Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a AUC-inspired metric to quantify performance longitudinally and sustainably.</p>
                </td>
              </tr>
            </tbody>
          </table>
          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- MLLM/LLM </heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
                  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="/BioMotion-Arena/static/images/overview.jpg" alt="BioMotion Arena" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2508.06072">
                    <papertitle>Can Large Models Fool the Eye? A New Turing Test for Biological Animation</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://openreview.net/profile?id=~Lirong_Deng1">Lirong Deng</a>, <a href="https://openreview.net/profile?id=~Zhengyu_Chen13">Zhengyu Chen</a>, <a href="https://openreview.net/profile?id=~Kaiwei_Zhang2">Kaiwei Zhang</a>, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=h-OG6DgAAAAJ">Qi Jia</a>, <a href="https://scholar.google.com.hk/citations?user=Kzd0qtsAAAAJ&hl=zh-CN">Yuan Tian</a>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Yucheng Zhu</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                 <br> 
                 <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://zijianchen98.github.io/BioMotion-Arena/">project page</a>
                  /
                  <a href="https://github.com/zijianchen98/BioMotion_Arena">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2508.06072">arXiv</a>
                  <br>
                  <p> In this paper, we introduce <strong>BioMotion Arena</strong>, the first biological motion-based visual preference evaluation framework for large models. We focus on ten typical human motions and introduce fine-grained control over gender, weight, mood, and direction. More than 45k votes for <strong>53</strong> mainstream LLMs and MLLMs on <strong>90</strong> biological motion variants are collected.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/LMM-JND.jpg" alt="LMM-JND" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2507.00490">
                    <papertitle>Just Noticeable Difference for Large Multimodal Models</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=Kzd0qtsAAAAJ&hl=zh-CN">Yuan Tian</a>, Yuze Sun, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=D_S41X4AAAAJ&hl=zh-CN">Weisi Lin</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                 <br> 
                 <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://zijianchen98.github.io/LMM-JND">project page</a>
                  /
                  <a href="https://github.com/zijianchen98/LMM-JND">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2507.00490">arXiv</a>
                  <br>
                  <p> In this paper, we propose a novel concept, <strong>LMM-JND</strong>, to quantify the perceptual redundancy characteristic for LMMs and a well-designed pipeline for its determination. We also construct a large-scale dataset, named <strong>VPA-JND</strong>, which contains 21.5k reference images with over 489k stimuli across 12 distortion types, to facilitate LMM-JND studies.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/PuzzleBench.png" alt="Puzzlebench" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2504.10885">
                    <papertitle>PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving</papertitle> 
                  </a>
                  <br>
                 <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=r2f885QAAAAJ">Zeyu Zhang*</a>, <strong>Zijian Chen*</strong>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Yuze Sun, <a href="https://scholar.google.com.hk/citations?user=Kzd0qtsAAAAJ&hl=zh-CN">Yuan Tian</a>, <a href="https://scholar.google.com.hk/citations?user=JYqad5sAAAAJ&hl=zh-CN&oi=sra">Ziheng Jia</a>, <a href="https://scholar.google.com.hk/citations?user=WosRriMAAAAJ&hl=zh-CN">Chunyi Li</a>, <a href="https://scholar.google.com.hk/citations?user=Tq2hoMQAAAAJ&hl=zh-CN">Xiaohong Liu</a>, <a href="https://minxiongkuo.github.io/">Xiongkuo Min</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai</a>
                  <br>
                  <em>arXiv</em>, 2025.
                  <br>
                  <a href="https://arxiv.org/abs/2504.10885">arXiv</a>
                  <br>
                  <p> In this paper, we construct <strong>PuzzleBench</strong>, a dynamic and scalable benchmark comprising 11,840 VQA samples, which features six carefully designed puzzle tasks targeting three core LMM competencies, visual recognition, logical reasoning, and context understanding.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- Image/Video Quality Assessment/Low-level </heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/debanding_framework.jpg" alt="debanding" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10896716">
                    <papertitle>Joint Luminance-Chrominance Learning for Image Debanding</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, Jun Jia, Ru Huang, Fangfang Lu, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=NpTmcKEAAAAJ">Ying Chen</a>, <a href="https://minxiongkuo.github.io/">Xiongkuo Min</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2025. 
                  <br>
                  <p> In this paper, we propose a unified deep neural network that explicitly disentangles the <strong>luminance</strong> and <strong>chrominance</strong> channels, and simultaneously recovers intensity gradients and color discontinuity from detection-free measurement in an end-to-end manner.</p>
                </td>
              </tr>
            </tbody>
          </table>
                  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/GAIA.jpg" alt="GAIA" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/46b5405a720a99db4c758cff43c8b4d3-Abstract-Datasets_and_Benchmarks_Track.html/">
                    <papertitle>GAIA: Rethinking Action Quality Assessment for AI-Generated Videos</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=Kzd0qtsAAAAJ&hl=zh-CN">Yuan Tian</a>, Jun Jia, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=lGt-KkwAAAAJ&hl=zh-CN&oi=ao">Jiarui Wang</a>, Ru Huang, <a href="https://minxiongkuo.github.io/">Xiongkuo Min<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>NeurIPS</em>, 2024. &nbsp; <font color="red"> <strong>(Spotlight Presentation)</strong></font>
                  <br>
                  <a href="https://github.com/zijianchen98/GAIA">project page</a>
                  /
                  <a href="https://pan.sjtu.edu.cn/web/share/6b8cf0e3af33feafe3e562eb862088df">dataset</a> extraction code: s277
                  /
                  <a href="https://zhuanlan.zhihu.com/p/704035511">中文版速递: 知乎</a>
                  <br>
                  <p> In this work, we construct <strong>GAIA</strong>, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel <strong>causal reasoning-based</strong> perspective, resulting in <strong>971,244</strong> ratings among <strong>9,180</strong> video-action pairs, and evaluate a suite of popular text-to-video models on their ability to generate visually rational actions.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/AGIN.jpg" alt="AGIN" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://github.com/zijianchen98/AGIN/">
                    <papertitle>Study of Subjective and Objective Naturalness Assessment of AI-Generated Images</papertitle>
                  </a>
                  <br>
                  <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=wth-VbMAAAAJ&hl=zh-CN">Haoning Wu</a>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Jun Jia, Ru Huang, <a href="https://minxiongkuo.github.io/">Xiongkuo Min<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2025.
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10771738">paper</a>
                  /
                  <a href="https://1drv.ms/u/c/0c2191cb01cbf002/EQLwywHLkSEggAzSCQAAAAABnn9ICRkAnM9YuSRtAWoDLQ?e=Jidlia">dataset</a>
                  /
                  <a href="https://github.com/zijianchen98/AGIN">code</a>
                  <br>
                  <p> In this work, we construct the AI-Generated Image Naturalness (<strong>AGIN</strong>) dataset and propose the Joint Objective Image Naturalness evaluaTor (<strong>JOINT</strong>) to automatically assess the naturalness of AIGIs that align with human opinions. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/BAND-2k.jpg" alt="Band2k" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10438477">
                    <papertitle>BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, Jun Jia, Fangfang Lu, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Jing Liu, Ru Huang, <a href="https://minxiongkuo.github.io/">Xiongkuo Min<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2024. 
                  <br>
                  <a href="https://github.com/zijianchen98/BAND-2k">project page</a>
                  /
                  <a href="https://1drv.ms/u/c/0c2191cb01cbf002/EQLwywHLkSEggAzTCQAAAAAB3NqW8zb7sAA1I6rA81qK4w?e=2rKO3I">dataset</a>
                  <br>
                  <p> In this work, we build the Banding Artifact Noticeable Database (<strong>BAND-2k</strong>), which consists of 2,000 banding images generated by 15 compression and quantization schemes.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/ISCAS2024.jpg" alt="fsband" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10438477">
                    <papertitle>FS-BAND: A frequency-sensitive banding detector</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Ru Huang, Fangfang Lu, <a href="https://minxiongkuo.github.io/">Xiongkuo Min</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>IEEE International Symposium on Circuits and Systems (ISCAS)</em>, 2024. &nbsp;
                  <br>
                  <a href="https://github.com/zijianchen98/BAND-2k">project page</a>
                  <br>
                  <p> In this paper, we develop a no-reference banding evaluator for banding detection and quality assessment by leveraging its frequency characteristics.</p>
                </td>
              </tr>
            </tbody>
          </table>

          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- Oracle Bone Inscriptions (OBI) Processing</heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="/images/PictOBI-20k.jpg" alt="PictOBI-20k" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2509.05773">
                    <papertitle>PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen*</strong>, Wenjie Hua*, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=cW4lv9IAAAAJ">Jinhao Li</a>, <a href="https://openreview.net/profile?id=~Lirong_Deng1">Lirong Deng</a>, Fan Du, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                 <br> 
                 <em>ICASSP</em>, 2026. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/OBI-Future/PictOBI-20k">Repo.</a>
                  /
                  <a href="https://arxiv.org/abs/2509.05773">Paper</a>
                  <br>
                  <p> In this paper, we introduce <strong>PictOBI-20k</strong>, a dataset designed to evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It includes 20k meticulously collected OBC and real object images, forming over 15k multi-choice questions. We also conduct subjective annotations to investigate the consistency of the reference point between humans and LMMs in visual reasoning.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/OBI-Bench.jpg" alt="obi-bench" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.01175">
                    <papertitle>OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                  <br>
                  <em>ICLR</em>, 2025. &nbsp; <font color="red"><strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/zijianchen98/OBI-Bench">project page</a>
                  /
                  <a href="https://pan.sjtu.edu.cn/web/share/a0c2912688b1097d300ec0c8de743545">dataset</a> extraction code: 7adv
                  /
                  <a href="https://zhuanlan.zhihu.com/p/10309270594">中文版速递: 知乎</a>
                  <br>
                  <p> In this work, we introduce <strong>OBI-Bench</strong>, a holistic benchmark crafted to systematically evaluate large multi-modal models (LMMs) on whole-process oracle bone inscriptions (OBI) processing tasks demanding expert-level domain knowledge and deliberate cognition.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/Oracle-P15k.png" alt="Oracle-P15k" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://github.com/OBI-Future/Oracle-P15K">
                    <papertitle>Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark</papertitle> 
                  </a>
                  <br>
                 <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=cW4lv9IAAAAJ">Jinhao Li*</a>, <strong>Zijian Chen*</strong>, Runze Jiang, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://faculty.ecnu.edu.cn/_s16/wzb/main.psp">Changbo Wang<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                  <br>
                  <em>ACM MM</em>, 2025. &nbsp; <font color="red"> <strong>(Oral Recommendation)</strong></font>
                  <br>
                  <a href="https://github.com/OBI-Future/Oracle-P15K">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2504.09555">arXiv</a>
                  <br>
                  <p> In this paper, we present the <strong>Oracle-P15K</strong>, a structure-aligned OBI dataset for OBI generation and denoising, consisting of 14,542 images infused with domain knowledge from OBI experts. Based on this, we propose a diffusion model-based pseudo OBI generator, called <strong>OBIDiff</strong>, to achieve realistic and controllable OBI generation.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/OBIFormer.jpg" alt="OBIFormer" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0141938225000964">
                    <papertitle>OBIFormer: A fast attentive denoising framework for oracle bone inscriptions</papertitle>
                  </a>
                  <br>
                 <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=cW4lv9IAAAAJ">Jinhao Li</a>, <strong>Zijian Chen</strong>, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://zyxy.ecnu.edu.cn/89/7b/c36124a428411/page.htm">Zhiji Liu</a>, <a href="https://faculty.ecnu.edu.cn/_s16/wzb/main.psp">Changbo Wang</a>
                  <br>
                  <em>Displays</em>, 2025.
                  <br>
                  <a href="https://github.com/LJHolyGround/OBIFormer">project page</a>
                  <br>
                  <p> In this work, we propose OBIFormer, a fast attentive framework for high-precision Oracle bone inscriptions denoising.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- Early Works</heading>
                  <p>
                    * denotes the sole student author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/iet-its.jpg" alt="STCGCN" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/itr2.12330">
                    <papertitle>Spatial-temporal correlation graph convolutional networks for traffic forecasting</papertitle>
                  </a>
                  <br>
                  Ru Huang (Master's Advisor), <strong>Zijian Chen*</strong>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai</a>, <a href="https://scholar.google.com.hk/citations?user=4SUAgfAAAAAJ&hl=zh-CN">Jianhua He</a>, <a href="https://scholar.google.com.hk/citations?user=kboVs84AAAAJ&hl=zh-CN">Xiaoli Chu</a>
                  <br>
                  <em>IET Intelligent Transport Systems</em>, 2023. &nbsp;
                  <br>
                  <p> In this work, we propose a novel architecture, named spatial-temporal correlation graph convolutional networks (STCGCN), for traffic prediction. </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/TNSE.jpg" alt="TNSE" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9928404">
                    <papertitle>A Graph Entropy Measure From Urelement to Higher-Order Graphlets for Network Analysis</papertitle>
                  </a>
                  <br>
                  Ru Huang (Master's Advisor), <strong>Zijian Chen*</strong>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai</a>, <a href="https://scholar.google.com.hk/citations?user=4SUAgfAAAAAJ&hl=zh-CN">Jianhua He</a>, <a href="https://scholar.google.com.hk/citations?user=kboVs84AAAAJ&hl=zh-CN">Xiaoli Chu</a>
                  <br>
                  <em>IEEE Transactions on Network Science and Engineering</em>, 2022. &nbsp;
                  <br>
                  <p> In this work, we introduce an unbiased graphlet estimation strategy to obtain both urelement and higher-order statistics for network analysis. </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Reviewer Service</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>  
                    <li> International Conference on Machine Learning (<strong>ICML</strong> <a href="https://icml.cc/Conferences/2026">2026</a>) </li>
                    <li> IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong> <a href="https://cvpr.thecvf.com/Conferences/2026">2026</a>)</li>
                    <li> Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong> <a href="https://neurips.cc/Conferences/2025">2025</a>) </li>
                    <li> International Conference on Learning Representations (<strong>ICLR</strong> <a href="https://iclr.cc/">2025, 2026</a>) </li>
                    <li> ACM Multimedia (<strong>ACM MM</strong> <a href="https://2025.acmmm.org/">2025</a>) </li>
                    <li> Annual AAAI Conference on Artificial Intelligence (<strong>AAAI</strong> <a href="https://aaai.org/conference/aaai/aaai-26/">2026</a>) </li>
                    <li> IEEE Transactions on Multimedia (<strong>TMM</strong> <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia">2025</a>) </li>
                    <li> ACM Transactions on Multimedia Computing Communications and Applications (<strong>ACM TOMM</strong> <a href="https://dl.acm.org/journal/tomm/">2025</a>) </li>
                    <li> IEEE Intelligent Transportation Systems Magazine (<strong>ITSM</strong> <a href="https://ieee-itss.org/pub/its-magazine/">2024</a>) </li>
                    <li> Journal of Supercomputing <a href="https://link.springer.com/journal/11227">2025</a></li>
                    <li> Displays <a href="https://www.journals.elsevier.com/displays">2025</a></li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Talks</heading>
                </td>
              </tr>
            </tbody>
          </table>
          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SJTU_logo.png" style="width:80%;max-width:200px">
                </td> -->
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>
                    <li> [2025.12] Large Multimodal Model-Driven Oracle Bone Inscriptions Information Processing (The 1st CCF AI for Humanities Conference)</li>
                    <li> [2024.11] AI+Virtual Simulation: Empowering Display Device Development (The 16th China Display Academic Conference) </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Blogs</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>
                    <li> [2025.06.11] <a href="https://zijianchen98.github.io/blog/20250611/">Gaining Inspiration from Darwinian Evolution: Towards Self-Referential and Self-Improving AI System</a></li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Awards</heading>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SJTU_logo.png" style="width:80%;max-width:200px">
                </td> -->
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>
                    <li> [2025] <strong>Young Science and Technology Scientists Sponsorship Program by CAST - Doctoral Student Special Plan </strong> (中国科协青年科技人才培育工程博士生专项计划)</li>
                    <li> [2025] <strong>National Scholarship</strong> (for PhD students) </li>
                    <li> [2023] <strong>Excellent Graduates in Shanghai</strong> (for postgraduates) </li>
                    <li> [2021] National Second Prize (National Graduate Electronics Design Contest) </li>
                    <li> [2019] First Prize in Zhejiang Province (National Undergraduate Electronics Design Contest) </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=p7iHbnJc-FcNcM90lORZz5_R574L4XTf-yVYZZtqIIY&cl=ffffff&w=a"></script>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <p style="text-align:left;font-size:small;"> Updated in Jan. 2026
                  </p>
                  <p style="text-align:right;font-size:small;">
                    Thanks <a href="https://jonbarron.info/"> Jon Barron</a> for this amazing website template</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>