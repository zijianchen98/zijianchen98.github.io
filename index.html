<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zijian Chen</title>

  <meta name="author" content="Zijian Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:1%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zijian Chen 「陈子健」</name>
                  </p>
                  <p>
                    I am currently a first-year PhD Student of <a href="https://multimedia.sjtu.edu.cn/">Multimedia Lab</a> at <a href="https://sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a>, advised by <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=24&infoid=66&flag=66">Prof. Guangtao Zhai</a> and <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Prof. Wenjun Zhang</a>.
                    Previously, I received my M.E. degree from <a href="https://www.ecust.edu.cn/">East China University of Science and Technology</a> in June 2023, and B.S. degree in EE from <a href="https://www.wzu.edu.cn/">Wenzhou University</a> in June 2020. </p>
                  <p>
                    I'm generally interested in <strong>image/video quality assessment</strong>, <strong>large multimodal models</strong>, especially <strong>all-round evaluation</strong>, and AI4Humanity (Oracle bone character processing).
                    My ultimate goal is to build models with human-like general intelligence that can seamlessly understand, generate, and reason across multiple modalities.
                  </p>  
                  <p>
                    <font color="red"> I'm always eager to communicate and cooperate, so feel free to contact me!!! </font>
                  </p>

                  <p>
                    Email: zijian.chen@sjtu.edu.cn  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   <!-- WeChat: xxxxx_ --> 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:zijian.chen@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <!-- <a href="data/xxxxx_CV.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?hl=en&user=NSR4UkMAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/zijianchen98/">Github</a> &nbsp/&nbsp
                    <a href="https://www.zhihu.com/people/amorzhu-ling-feng">Zhihu</a>
                    <!-- <a href="https://www.linkedin.com/in/xxxxx/">LinkedIn</a> -->
                  </p>
                </td>
                <td style="padding:1%;width:30%;max-width:30%">
                  <a href="images/czj.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/czj.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> [2025.04] Start internship at <strong><a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a></strong> as a research intern, Shanghai (on-site). </li>
                    <li> [2025.04] I will attend <strong><a href="https://iclr.cc/">ICLR 2025</a></strong> in person. See you in Singapore! </li>
                    <li> [2025.02] One paper about (<strong><a href="https://ieeexplore.ieee.org/abstract/document/10896716">image debanding</a></strong>) has been accepted by <strong><a href="https://ieee-cas.org/publication/tcsvt">IEEE TCSVT 2025</a></strong>. </li>
                    <li> [2025.01] One paper (<strong><a href="https://github.com/zijianchen98/OBI-Bench">OBI-Bench</a></strong>) has been accepted by <strong><a href="https://iclr.cc/">ICLR 2025</a></strong>. </li>
                    <li> [2024.11] One paper (<strong><a href="https://github.com/zijianchen98/AGIN">AGIN</a></strong>), has been accepted by <strong><a href="https://ieee-cas.org/publication/tcsvt">IEEE TCSVT 2025</a></strong>. </li>
                    <li> [2024.09] One paper (<strong><a href="https://github.com/zijianchen98/GAIA">GAIA</a></strong>) has been accepted by <strong><a href="https://neurips.cc/Conferences/2024">NeurIPS 2024 D&B Track</a></strong> and selected as <strong>Spotlight</strong>. </li>
                    <li> [2024.02] One paper (<a href="https://ieeexplore.ieee.org/abstract/document/10438477">BAND-2k</a>) has been accepted by <a href="https://ieee-cas.org/publication/tcsvt">IEEE TCSVT 2024</a>. </li>
                    <li> [2024.01] One paper (<a href="https://ieeexplore.ieee.org/abstract/document/10558429">FS-BAND</a>) has been accepted by <a href="https://2024.ieee-iscas.org/">ISCAS 2024</a>. </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Preprints</heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/PuzzleBench.png" alt="Puzzlebench" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2504.10885">
                    <papertitle>PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving</papertitle> 
                  </a>
                  <br>
                 <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=r2f885QAAAAJ">Zeyu Zhang*</a>, <strong>Zijian Chen*</strong>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Yuze Sun, <a href="https://scholar.google.com.hk/citations?user=Kzd0qtsAAAAJ&hl=zh-CN">Yuan Tian</a>, <a href="https://scholar.google.com.hk/citations?user=JYqad5sAAAAJ&hl=zh-CN&oi=sra">Ziheng Jia</a>, <a href="https://scholar.google.com.hk/citations?user=WosRriMAAAAJ&hl=zh-CN">Chunyi Li</a>, <a href="https://scholar.google.com.hk/citations?user=Tq2hoMQAAAAJ&hl=zh-CN">Xiaohong Liu</a>, <a href="https://minxiongkuo.github.io/">Xiongkuo Min</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai</a>
                  <br>
                  <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2504.10885">arXiv</a>
                  <br>
                  <p> In this paper, we construct <strong>PuzzleBench</strong>, a dynamic and scalable benchmark comprising 11,840 VQA samples, which features six carefully designed puzzle tasks targeting three core LMM competencies, visual recognition, logical reasoning, and context understanding.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/Oracle-P15k.png" alt="Oracle-P15k" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://github.com/OBI-Future/Oracle-P15K">
                    <papertitle>Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark</papertitle> 
                  </a>
                  <br>
                 <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=cW4lv9IAAAAJ">Jinhao Li*</a>, <strong>Zijian Chen*</strong>, Jiangrun Ze, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://faculty.ecnu.edu.cn/_s16/wzb/main.psp">Changbo Wang<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                  <br>
                  <em>arXiv</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/OBI-Future/Oracle-P15K">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2504.09555">arXiv</a>
                  <br>
                  <p> In this paper, we present the <strong>Oracle-P15K</strong>, a structure-aligned OBI dataset for OBI generation and denoising, consisting of 14,542 images infused with domain knowledge from OBI experts. Based on this, we propose a diffusion model-based pseudo OBI generator, called <strong>OBIDiff</strong>, to achieve realistic and controllable OBI generation.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- MLLM/LLM </heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
                  
          -----


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- Image/Video Quality Assessment/Low-level </heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/debanding_framework.jpg" alt="debanding" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10896716">
                    <papertitle>Joint Luminance-Chrominance Learning for Image Debanding</papertitle> 
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, Jun Jia, Ru Huang, Fangfang Lu, <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=NpTmcKEAAAAJ">Ying Chen</a>, <a href="https://minxiongkuo.github.io/">Xiongkuo Min</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <p> In this paper, we propose a unified deep neural network that explicitly disentangles the <strong>luminance</strong> and <strong>chrominance</strong> channels, and simultaneously recovers intensity gradients and color discontinuity from detection-free measurement in an end-to-end manner.</p>
                </td>
              </tr>
            </tbody>
          </table>
                  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/GAIA.jpg" alt="GAIA" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/46b5405a720a99db4c758cff43c8b4d3-Abstract-Datasets_and_Benchmarks_Track.html/">
                    <papertitle>GAIA: Rethinking Action Quality Assessment for AI-Generated Videos</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=Kzd0qtsAAAAJ&hl=zh-CN">Yuan Tian</a>, Jun Jia, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=lGt-KkwAAAAJ&hl=zh-CN&oi=ao">Jiarui Wang</a>, Ru Huang, <a href="https://minxiongkuo.github.io/">Xiongkuo Min<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>NeurIPS</em>, 2024. &nbsp; <font color="red"> <strong>(Spotlight Presentation)</strong></font>
                  <br>
                  <a href="https://github.com/zijianchen98/GAIA">project page</a>
                  /
                  <a href="https://pan.sjtu.edu.cn/web/share/6b8cf0e3af33feafe3e562eb862088df">dataset</a> extraction code: s277
                  /
                  <a href="https://zhuanlan.zhihu.com/p/704035511">中文版速递: 知乎</a>
                  <br>
                  <p> In this work, we construct <strong>GAIA</strong>, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel <strong>causal reasoning-based</strong> perspective, resulting in <strong>971,244</strong> ratings among <strong>9,180</strong> video-action pairs, and evaluate a suite of popular text-to-video models on their ability to generate visually rational actions.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/AGIN.jpg" alt="AGIN" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://github.com/zijianchen98/AGIN/">
                    <papertitle>Study of Subjective and Objective Naturalness Assessment of AI-Generated Images</papertitle>
                  </a>
                  <br>
                  <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=wth-VbMAAAAJ&hl=zh-CN">Haoning Wu</a>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Jun Jia, Ru Huang, <a href="https://minxiongkuo.github.io/">Xiongkuo Min<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2025. &nbsp; <font color="red"> <strong>(NEW)</strong></font>
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10771738">paper</a>
                  /
                  <a href="https://1drv.ms/u/c/0c2191cb01cbf002/EQLwywHLkSEggAzSCQAAAAABnn9ICRkAnM9YuSRtAWoDLQ?e=Jidlia">dataset</a>
                  /
                  <a href="https://github.com/zijianchen98/AGIN">code</a>
                  <br>
                  <p> In this work, we construct the AI-Generated Image Naturalness (<strong>AGIN</strong>) dataset and propose the Joint Objective Image Naturalness evaluaTor (<strong>JOINT</strong>) to automatically assess the naturalness of AIGIs that align with human opinions. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/BAND-2k.jpg" alt="Band2k" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10438477">
                    <papertitle>BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, Jun Jia, Fangfang Lu, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Jing Liu, Ru Huang, <a href="https://minxiongkuo.github.io/">Xiongkuo Min<sup>†</sup></a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                  <br>
                  <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, 2024. &nbsp;
                  <br>
                  <a href="https://github.com/zijianchen98/BAND-2k">project page</a>
                  /
                  <a href="https://1drv.ms/u/c/0c2191cb01cbf002/EQLwywHLkSEggAzTCQAAAAAB3NqW8zb7sAA1I6rA81qK4w?e=2rKO3I">dataset</a>
                  <br>
                  <p> In this work, we build the Banding Artifact Noticeable Database (<strong>BAND-2k</strong>), which consists of 2,000 banding images generated by 15 compression and quantization schemes.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/ISCAS2024.jpg" alt="fsband" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/10438477">
                    <papertitle>FS-BAND: A frequency-sensitive banding detector</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://scholar.google.com.hk/citations?user=nDlEBJ8AAAAJ&hl=zh-CN">Wei Sun</a>, <a href="https://scholar.google.com.hk/citations?user=QICTEckAAAAJ&hl=zh-CN">Zicheng Zhang</a>, Ru Huang, Fangfang Lu, <a href="https://minxiongkuo.github.io/">Xiongkuo Min</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>
                  <br>
                  <em>IEEE International Symposium on Circuits and Systems (ISCAS)</em>, 2024. &nbsp;
                  <br>
                  <a href="https://github.com/zijianchen98/BAND-2k">project page</a>
                  <br>
                  <p> In this paper, we develop a no-reference banding evaluator for banding detection and quality assessment by leveraging its frequency characteristics.</p>
                </td>
              </tr>
            </tbody>
          </table>

          

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- Oracle Bone Inscriptions (OBI) Processing</heading>
                  <p>
                    * denotes equal contribution, and <sup>†</sup> denotes corresponding author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/OBI-Bench.jpg" alt="obi-bench" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.01175">
                    <papertitle>OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?</papertitle>
                  </a>
                  <br>
                 <strong>Zijian Chen</strong>, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://ee.sjtu.edu.cn/FacultyDetail.aspx?id=14&infoid=66&flag=66">Wenjun Zhang</a>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai<sup>†</sup></a>
                  <br>
                  <em>ICLR</em>, 2025. &nbsp; <font color="red"><strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/zijianchen98/OBI-Bench">project page</a>
                  /
                  <a href="https://pan.sjtu.edu.cn/web/share/a0c2912688b1097d300ec0c8de743545">dataset</a> extraction code: 7adv
                  /
                  <a href="https://zhuanlan.zhihu.com/p/10309270594">中文版速递: 知乎</a>
                  <br>
                  <p> In this work, we introduce <strong>OBI-Bench</strong>, a holistic benchmark crafted to systematically evaluate large multi-modal models (LMMs) on whole-process oracle bone inscriptions (OBI) processing tasks demanding expert-level domain knowledge and deliberate cognition.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/OBIFormer.jpg" alt="OBIFormer" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0141938225000964">
                    <papertitle>OBIFormer: A fast attentive denoising framework for oracle bone inscriptions</papertitle>
                  </a>
                  <br>
                 <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=cW4lv9IAAAAJ">Jinhao Li</a>, <strong>Zijian Chen</strong>, <a href="https://shss.sjtu.edu.cn/Web/FacultyDetail/46?f=1&t=4">Tingzhu Chen<sup>†</sup></a>, <a href="https://zyxy.ecnu.edu.cn/89/7b/c36124a428411/page.htm">Zhiji Liu</a>, <a href="https://faculty.ecnu.edu.cn/_s16/wzb/main.psp">Changbo Wang</a>
                  <br>
                  <em>Displays</em>, 2025. &nbsp; <font color="red"><strong>(NEW)</strong></font>
                  <br>
                  <a href="https://github.com/LJHolyGround/OBIFormer">project page</a>
                  <br>
                  <p> In this work, we propose OBIFormer, a fast attentive framework for high-precision Oracle bone inscriptions denoising.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Publications -- Early Works</heading>
                  <p>
                    * denotes the sole student author.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/iet-its.jpg" alt="STCGCN" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/itr2.12330">
                    <papertitle>Spatial-temporal correlation graph convolutional networks for traffic forecasting</papertitle>
                  </a>
                  <br>
                  Ru Huang (Master's Advisor), <strong>Zijian Chen*</strong>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai</a>, <a href="https://scholar.google.com.hk/citations?user=4SUAgfAAAAAJ&hl=zh-CN">Jianhua He</a>, <a href="https://scholar.google.com.hk/citations?user=kboVs84AAAAJ&hl=zh-CN">Xiaoli Chu</a>
                  <br>
                  <em>IET Intelligent Transport Systems</em>, 2023. &nbsp;
                  <br>
                  <p> In this work, we propose a novel architecture, named spatial-temporal correlation graph convolutional networks (STCGCN), for traffic prediction. </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/TNSE.jpg" alt="TNSE" width="200">
                </td>
                <td style="padding:5px;width:70%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9928404">
                    <papertitle>A Graph Entropy Measure From Urelement to Higher-Order Graphlets for Network Analysis</papertitle>
                  </a>
                  <br>
                  Ru Huang (Master's Advisor), <strong>Zijian Chen*</strong>, <a href="https://scholar.google.com.hk/citations?user=E6zbSYgAAAAJ&hl=zh-CN">Guangtao Zhai</a>, <a href="https://scholar.google.com.hk/citations?user=4SUAgfAAAAAJ&hl=zh-CN">Jianhua He</a>, <a href="https://scholar.google.com.hk/citations?user=kboVs84AAAAJ&hl=zh-CN">Xiaoli Chu</a>
                  <br>
                  <em>IEEE Transactions on Network Science and Engineering</em>, 2022. &nbsp;
                  <br>
                  <p> In this work, we introduce an unbiased graphlet estimation strategy to obtain both urelement and higher-order statistics for network analysis. </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Reviewer Service</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>  
                    <li> Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong> <a href="https://neurips.cc/Conferences/2025">2025</a>) </li>
                    <li> International Conference on Learning Representations (<strong>ICLR</strong> <a href="https://iclr.cc/">2025</a>) </li>
                    <li> ACM Multimedia (<strong>ACM MM</strong> <a href="https://2025.acmmm.org/">2025</a>) </li>
                    <li> IEEE Transactions on Multimedia (<strong>TMM</strong> <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia">2025</a>) </li>
                    <li> IEEE Intelligent Transportation Systems Magazine (<strong>ITSM</strong> <a href="https://ieee-itss.org/pub/its-magazine/">2024</a>) </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Talks</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SJTU_logo.png" style="width:80%;max-width:200px">
                </td> -->
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>
                    <li> [2024.11] <strong>AI+Virtual Simulation: Empowering Display Device Development</strong> (The 16th China Display Academic Conference) </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Awards</heading>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <!-- <td style="padding:5px;width:28%;vertical-align:middle">
                  <img src="images/SJTU_logo.png" style="width:80%;max-width:200px">
                </td> -->
                <td style="padding:0px;width:75%;vertical-align:middle">
                  <ul>
                    <li> [2023] <strong>Excellent Graduates in Shanghai</strong> (for postgraduates) </li>
                    <li> [2021] National Second Prize (National Graduate Electronics Design Contest) </li>
                    <li> [2019] First Prize in Zhejiang Province (National Undergraduate Electronics Design Contest) </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=p7iHbnJc-FcNcM90lORZz5_R574L4XTf-yVYZZtqIIY&cl=ffffff&w=a"></script>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <p style="text-align:left;font-size:small;"> Updated in Jun. 2025
                  </p>
                  <p style="text-align:right;font-size:small;">
                    Thanks <a href="https://jonbarron.info/"> Jon Barron</a> for this amazing website template</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>